{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1bwUnj-9NnJA2EMr7eWO4I45UuBtKudg_","timestamp":1671155282905}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HFLXe4HasHgr"},"source":["# Motion Transfer\n","\n","Credits: https://github.com/iPERDance/iPERCore"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vPl00TgplS-","executionInfo":{"status":"ok","timestamp":1671155375108,"user_tz":360,"elapsed":3228,"user":{"displayName":"Alex Zhang","userId":"09441695657426098781"}},"outputId":"9d09d305-a17f-407b-f2c0-7ad18884512a"},"source":["# Install ffmpeg (ffprobe)\n","!apt-get install ffmpeg"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q45P7Uicpsuc","executionInfo":{"status":"ok","timestamp":1671155375109,"user_tz":360,"elapsed":11,"user":{"displayName":"Alex Zhang","userId":"09441695657426098781"}},"outputId":"1578a05b-e89d-4151-bb50-112f54c5486e"},"source":["# set CUDA_HOME, here we use CUDA 10.1\n","import os\n","os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-10.1\"\n","\n","!echo $CUDA_HOME"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/cuda-10.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"uYAJxIf9xBxU"},"source":["## 1.1 Clone iPERCore Github Repo"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5J6oiRq1xKvm","executionInfo":{"status":"ok","timestamp":1671155379928,"user_tz":360,"elapsed":3620,"user":{"displayName":"Alex Zhang","userId":"09441695657426098781"}},"outputId":"bc1cdb23-d7ad-42cb-b9d2-48bd455f36bf"},"source":["!git clone https://github.com/iPERDance/iPERCore.git"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'iPERCore'...\n","remote: Enumerating objects: 902, done.\u001b[K\n","remote: Counting objects: 100% (38/38), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 902 (delta 17), reused 29 (delta 14), pack-reused 864\u001b[K\n","Receiving objects: 100% (902/902), 23.05 MiB | 13.80 MiB/s, done.\n","Resolving deltas: 100% (334/334), done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"4Y9-b45uwx4f"},"source":["## 1.2 Setup iPERCore"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8aXNebVxv72E","executionInfo":{"status":"ok","timestamp":1667147293341,"user_tz":-480,"elapsed":3,"user":{"displayName":"Wen Liu","userId":"05258915793689473125"}},"outputId":"af23e736-f9ae-4c7b-fb55-f4438e372a17"},"source":["cd /content/iPERCore/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/iPERCore\n"]}]},{"cell_type":"code","metadata":{"id":"eZAZlLRHH2lq"},"source":["!python setup.py develop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqFnPDBHN5WO"},"source":["# Download all checkpoints\n","\n","!wget -O assets/checkpoints.zip \"http://101.32.75.151:10086/checkpoints.zip\"\n","!unzip -o assets/checkpoints.zip -d assets/\n","\n","!rm assets/checkpoints.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-_QjX1FzsEQI"},"source":["# download samples\n","# !wget -O assets/samples.zip \"https://1drv.ws/u/s!AjjUqiJZsj8whLobQPpoxo2hfhURrA?e=EUyIC2\"\n","!wget -O assets/samples.zip  \"http://101.32.75.151:12345/iper_plus_plus_latest_samples.zip\"\n","!unzip -o assets/samples.zip -d  assets\n","!rm assets/samples.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9KymQM8z6sH","executionInfo":{"elapsed":284,"status":"ok","timestamp":1656262549826,"user":{"displayName":"Wen Liu","userId":"05258915793689473125"},"user_tz":-480},"outputId":"7da2b222-1781-4260-b0dc-14c0d1ddac94"},"source":["cd /content/iPERCore/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/iPERCore\n"]}]},{"cell_type":"code","metadata":{"id":"BJskZDTiEKMB"},"source":["import os\n","import os.path as osp\n","import platform\n","import argparse\n","import time\n","import sys\n","import subprocess\n","from IPython.display import HTML\n","from base64 import b64encode"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lo9IgZwsOmBd"},"source":["## Details of Config\n"," - gpu_ids (str): the gpu_ids, default is \"0\";\n"," - image_size (int): the image size, default is 512;\n"," - num_source (int): the number of source images for Attention, default is 2. Large needs more GPU memory;\n"," - assets_dir (str): the assets directory. This is very important, and there are the configurations and all pre-trained checkpoints;\n"," - output_dir (str): the output directory;\n","\n"," - src_path (str): the source input information. \n","       All source paths and it supports multiple paths, uses \"|\" as the separator between all paths. \n","       The format is \"src_path_1|src_path_2|src_path_3\". \n","       \n","       Each src_input is \"path?=path1,name?=name1,bg_path?=bg_path1\". \n","       \n","       It must contain 'path'. If 'name' and 'bg_path' are empty, they will be ignored.\n","\n","       The 'path' could be an image path, a path of a directory contains source images, and a video path.\n","\n","       The 'name' is the rename of this source input, if it is empty, we will ignore it, and use the filename of the path.\n","\n","       The 'bg_path' is the actual background path if provided, otherwise we will ignore it.\n","       \n","       There are several examples of formated source paths,\n","\n","        1. \"path?=path1,name?=name1,bg_path?=bg_path1|path?=path2,name?=name2,bg_path?=bg_path2\",\n","        this input will be parsed as [{path: path1, name: name1, bg_path:bg_path1},\n","        {path: path2, name: name2, bg_path: bg_path2}];\n","\n","        2. \"path?=path1,name?=name1|path?=path2,name?=name2\", this input will be parsed as\n","        [{path: path1, name:name1}, {path: path2, name: name2}];\n","\n","        3. \"path?=path1\", this input will be parsed as [{path: path1}].\n","\n","        4. \"path1\", this will be parsed as [{path: path1}].\n","\n"," - ref_path (str): the reference input information.\n","       \n","       All reference paths. It supports multiple paths, and uses \"|\" as the separator between all paths.\n","       The format is \"ref_path_1|ref_path_2|ref_path_3\".\n","\n","       Each ref_path is \"path?=path1,name?=name1,audio?=audio_path1,fps?=30,pose_fc?=300,cam_fc?=150\".\n","\n","       It must contain 'path', and others could be empty, and they will be ignored.\n","\n","       The 'path' could be an image path, a path of a directory contains images of a same person, and a video path.\n","\n","       The 'name' is the rename of this source input, if it is empty, we will ignore it, and use the filename of the path.\n","\n","       The 'audio' is the audio path, if it is empty, we will ignore it. If the 'path' is a video,\n","        you can ignore this, and we will firstly extract the audio information of this video (if it has audio channel).\n","\n","       The 'fps' is fps of the final outputs, if it is empty, we will set it as the default fps 25.\n","\n","       The 'pose_fc' is the smooth factor of the temporal poses. The smaller of this value, the smoother of the temporal poses. If it is empty, we will set it as the default 300. In the most cases, using the default 300 is enough, and if you find the poses of the outputs are not stable, you can decrease this value. Otherwise, if you find the poses of the outputs are over stable, you can increase this value.\n","\n","       The 'cam_fc' is the smooth factor of the temporal cameras (locations in the image space). The smaller of this value, the smoother of the locations in sequences. If it is empty, we will set it as the default 150. In the most cases, the default 150 is enough.\n","\n","       There are several examples of formated reference paths,\n","\n","        1. \"path?=path1,name?=name1,audio?=audio_path1,fps?=30,pose_fc?=300,cam_fc?=150|\n","            path?=path2,name?=name2,audio?=audio_path2,fps?=25,pose_fc?=450,cam_fc?=200\",\n","            this input will be parsed as\n","            [{path: path1, name: name1, audio: audio_path1, fps: 30, pose_fc: 300, cam_fc: 150},\n","             {path: path2, name: name2, audio: audio_path2, fps: 25, pose_fc: 450, cam_fc: 200}]\n","\n","        2. \"path?=path1,name?=name1, pose_fc?=450|path?=path2,name?=name2\", this input will be parsed as\n","        [{path: path1, name: name1, fps: 25, pose_fc: 450, cam_fc: 150},\n","         {path: path2, name: name2, fps: 25, pose_fc: 300, cam_fc: 150}].\n","\n","        3. \"path?=path1|path?=path2\", this input will be parsed as\n","        [{path: path1, fps:25, pose_fc: 300, cam_fc: 150}, {path: path2, fps: 25, pose_fc: 300, cam_fc: 150}].\n","\n","        4. \"path1|path2\", this input will be parsed as\n","        [{path: path1, fps:25, pose_fc: 300, cam_fc: 150}, {path: path2, fps: 25, pose_fc: 300, cam_fc: 150}].\n","\n","        5. \"path1\", this will be parsed as [{path: path1, fps: 25, pose_fc: 300, cam_fc: 150}]."]},{"cell_type":"code","metadata":{"id":"QNWVyAaeOhHP"},"source":["# the gpu ids\n","gpu_ids = \"0\"\n","\n","# the image size\n","image_size = 512\n","\n","# the default number of source images, it will be updated if the actual number of sources <= num_source\n","num_source = 1\n","\n","# the assets directory. This is very important, please download it from `one_drive_url` firstly.\n","assets_dir = \"/content/iPERCore/assets\"\n","\n","# the output directory.\n","output_dir = \"./results\"\n","\n","# the model id of this case. This is a random model name.\n","# model_id = \"model_\" + str(time.time())\n","\n","# # This is a specific model name, and it will be used if you do not change it.\n","# model_id = \"axing_1\"\n","\n","# symlink from the actual assets directory to this current directory\n","work_asserts_dir = os.path.join(\"./assets\")\n","if not os.path.exists(work_asserts_dir):\n","    os.symlink(osp.abspath(assets_dir), osp.abspath(work_asserts_dir),\n","               target_is_directory=(platform.system() == \"Windows\"))\n","\n","cfg_path = osp.join(work_asserts_dir, \"configs\", \"deploy.toml\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETaQCD0t1qqO"},"source":["# This is a specific model name, and it will be used if you do not change it. This is the case of `trump`\n","model_id = \"orange_justice\"\n","\n","# the source input information, here \\\" is escape character of double duote \"\n","src_path = \"\\\"path?=/content/iPERCore/assets/samples/sources/orange_justice\\\"\"\n","\n","\n","\n","ref_path = \"\\\"path?=/content/iPERCore/assets/samples/references/mabaoguo_short.mp4,\" \\\n","             \"name?=mabaoguo_short,\" \\\n","             \"pose_fc?=400\\\"\"\n","\n","!python -m iPERCore.services.run_imitator  \\\n","  --gpu_ids     $gpu_ids       \\\n","  --num_source  $num_source    \\\n","  --image_size  $image_size    \\\n","  --output_dir  $output_dir    \\\n","  --model_id    $model_id      \\\n","  --cfg_path    $cfg_path      \\\n","  --src_path    $src_path      \\\n","  --ref_path    $ref_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kSoXbrc6_81W","executionInfo":{"elapsed":983917,"status":"ok","timestamp":1656264371399,"user":{"displayName":"Wen Liu","userId":"05258915793689473125"},"user_tz":-480},"outputId":"4de96ba5-4dfd-4e2e-f07d-d5fc3f602ba1"},"source":["# # This is a specific model name, and it will be used if you do not change it. This is the case of `trump`\n","# model_id = \"axing_1\"\n","\n","# # the source input information, here \\\" is escape character of double duote \"\n","# src_path = \"\\\"path?=/content/iPERCore/assets/samples/sources/axing_1,name?=axing_1\\\"\"\n","\n","\n","# ## the reference input information. There are three reference videos in this case.\n","# # here \\\" is escape character of double duote \"\n","# ref_path = \"\\\"path?=/content/iPERCore/assets/samples/references/bantangzhuyi_1.mp4,\" \\\n","#              \"name?=bantangzhuyi_1,\" \\\n","#              \"pose_fc?=300\\\"\"\n","\n","# print(ref_path)\n","\n","# !python -m iPERCore.services.run_imitator  \\\n","#   --gpu_ids     $gpu_ids       \\\n","#   --num_source  $num_source    \\\n","#   --image_size  $image_size    \\\n","#   --output_dir  $output_dir    \\\n","#   --model_id    $model_id      \\\n","#   --cfg_path    $cfg_path      \\\n","#   --src_path    $src_path      \\\n","#   --ref_path    $ref_path"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\"path?=/content/iPERCore/assets/samples/references/bantangzhuyi_1.mp4,name?=bantangzhuyi_1,pose_fc?=300\"\n","ffprobe -show_entries stream=codec_type -of json /content/iPERCore/assets/samples/references/bantangzhuyi_1.mp4 -loglevel error\n","ffmpeg -y -i /content/iPERCore/assets/samples/references/bantangzhuyi_1.mp4 -ab 160k -ac 2 -ar 44100 -vn ./results/primitives/bantangzhuyi_1/processed/audio.mp3 -loglevel quiet\n","ffprobe -v error -select_streams v -of default=noprint_wrappers=1:nokey=1 -show_entries stream=r_frame_rate /content/iPERCore/assets/samples/references/bantangzhuyi_1.mp4\n","\tPre-processing: start...\n","----------------------MetaProcess----------------------\n","meta_input:\n","\tpath: /content/iPERCore/assets/samples/sources/axing_1\n","\tbg_path: \n","\tname: axing_1\n","primitives_dir: ./results/primitives/axing_1\n","processed_dir: ./results/primitives/axing_1/processed\n","vid_info_path: ./results/primitives/axing_1/processed/vid_info.pkl\n","-------------------------------------------------------\n","----------------------MetaProcess----------------------\n","meta_input:\n","\tpath: /content/iPERCore/assets/samples/references/bantangzhuyi_1.mp4\n","\tbg_path: \n","\tname: bantangzhuyi_1\n","\taudio: ./results/primitives/bantangzhuyi_1/processed/audio.mp3\n","\tfps: 30.0\n","\tpose_fc: 300.0\n","\tcam_fc: 100\n","\teffect: \n","primitives_dir: ./results/primitives/bantangzhuyi_1\n","processed_dir: ./results/primitives/bantangzhuyi_1/processed\n","vid_info_path: ./results/primitives/bantangzhuyi_1/processed/vid_info.pkl\n","-------------------------------------------------------\n","\t1.1 Preprocessing, running Preprocessor to detect the human boxes of ./results/primitives/axing_1/processed/orig_images...\n","100% 2/2 [00:00<00:00,  8.20it/s]\n","\t1.1 Preprocessing, finish detect the human boxes of ./results/primitives/axing_1/processed/orig_images ...\n","\t1.2 Preprocessing, cropping all images in ./results/primitives/axing_1/processed/orig_images by estimated boxes ...\n","2it [00:00, 47.09it/s]\n","\t1.2 Preprocessing, finish crop the human by boxes, and save them in ./results/primitives/axing_1/processed/images ...\n","\t1.3 Preprocessing, running Preprocessor to 3D pose estimation of all images in./results/primitives/axing_1/processed/images ...\n","100% 1/1 [00:00<00:00,  1.59it/s]\n","\t1.3 Preprocessing, finish 3D pose estimation successfully ....\n","\t1.4 Preprocessing, running Preprocessor to run human matting in ./results/primitives/axing_1/processed/parse ... \n","  0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n","100% 2/2 [00:00<00:00,  4.45it/s]\n","\t1.4 Preprocessing, finish run human matting.\n","\t1.5 Preprocessing, running Preprocessor to find 25 candidates front images in ./results/primitives/axing_1/processed/images ...\n","100% 2/2 [00:00<00:00, 33.16it/s]\n","\t1.5 Preprocessing, finish find the front images ....\n","\t1.6 Preprocessing, running Preprocessor to run background inpainting ...\n","100% 2/2 [00:01<00:00,  1.46it/s]\n","\t1.6 Preprocessing, finish run background inpainting ....\n","\t1.7 Preprocessing, saving visualization to ./results/primitives/axing_1/processed/visual.mp4 ...\n","100% 2/2 [00:01<00:00,  1.78it/s]\n","ffmpeg -y -i ./results/primitives/axing_1/processed/visual.mp4.avi -vcodec h264 ./results/primitives/axing_1/processed/visual.mp4 -loglevel quiet\n","\t1.7 Preprocessing, saving visualization to ./results/primitives/axing_1/processed/visual.mp4 ...\n","Preprocessor has finished...\n","/content/iPERCore/assets/samples/references/bantangzhuyi_1.mp4 Writing frames to file\n","ffmpeg -i /content/iPERCore/assets/samples/references/bantangzhuyi_1.mp4 -start_number 0 ./results/primitives/bantangzhuyi_1/processed/orig_images/frame_%08d.png\n","ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n","  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n","  libavutil      55. 78.100 / 55. 78.100\n","  libavcodec     57.107.100 / 57.107.100\n","  libavformat    57. 83.100 / 57. 83.100\n","  libavdevice    57. 10.100 / 57. 10.100\n","  libavfilter     6.107.100 /  6.107.100\n","  libavresample   3.  7.  0 /  3.  7.  0\n","  libswscale      4.  8.100 /  4.  8.100\n","  libswresample   2.  9.100 /  2.  9.100\n","  libpostproc    54.  7.100 / 54.  7.100\n","Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/iPERCore/assets/samples/references/bantangzhuyi_1.mp4':\n","  Metadata:\n","    major_brand     : isom\n","    minor_version   : 512\n","    compatible_brands: isomiso2avc1mp41\n","    encoder         : Lavf58.45.100\n","  Duration: 00:00:59.73, start: 0.000000, bitrate: 1929 kb/s\n","    Stream #0:0(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1920x1080 [SAR 1:1 DAR 16:9], 1792 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)\n","    Metadata:\n","      handler_name    : VideoHandler\n","    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s (default)\n","    Metadata:\n","      handler_name    : SoundHandler\n","Stream mapping:\n","  Stream #0:0 -> #0:0 (h264 (native) -> png (native))\n","Press [q] to stop, [?] for help\n","Output #0, image2, to './results/primitives/bantangzhuyi_1/processed/orig_images/frame_%08d.png':\n","  Metadata:\n","    major_brand     : isom\n","    minor_version   : 512\n","    compatible_brands: isomiso2avc1mp41\n","    encoder         : Lavf57.83.100\n","    Stream #0:0(eng): Video: png, rgb24, 1920x1080 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 30 fps, 30 tbn, 30 tbc (default)\n","    Metadata:\n","      handler_name    : VideoHandler\n","      encoder         : Lavc57.107.100 png\n","frame= 1791 fps= 15 q=-0.0 Lsize=N/A time=00:00:59.70 bitrate=N/A speed=0.505x    \n","video:1293762kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n","\t1.1 Preprocessing, running Preprocessor to detect the human boxes of ./results/primitives/bantangzhuyi_1/processed/orig_images...\n","100% 1791/1791 [04:09<00:00,  7.17it/s]\n","\t1.1 Preprocessing, finish detect the human boxes of ./results/primitives/bantangzhuyi_1/processed/orig_images ...\n","\t1.2 Preprocessing, cropping all images in ./results/primitives/bantangzhuyi_1/processed/orig_images by estimated boxes ...\n","1791it [00:42, 42.12it/s]\n","\t1.2 Preprocessing, finish crop the human by boxes, and save them in ./results/primitives/bantangzhuyi_1/processed/images ...\n","\t1.3 Preprocessing, running Preprocessor to 3D pose estimation of all images in./results/primitives/bantangzhuyi_1/processed/images ...\n","100% 56/56 [00:32<00:00,  1.73it/s]\n","\t1.3 Preprocessing, finish 3D pose estimation successfully ....\n","Preprocessor has finished...\n","\t\tPre-processing: digital deformation start...\n","Process HumanDigitalDeformConsumer_0:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n","    self.run()\n","  File \"/content/iPERCore/iPERCore/services/preprocess.py\", line 129, in run\n","    part_path=self.opt.part_path\n","  File \"/content/iPERCore/iPERCore/tools/human_digitalizer/deformers/clothlinks_deformer.py\", line 21, in __init__\n","    self.smpl_link = SmplLinker(smpl_model=smpl_model, part_path=part_path, device=device)\n","  File \"/content/iPERCore/iPERCore/tools/human_digitalizer/deformers/link_utils.py\", line 78, in __init__\n","    self.left_leg_verts_idx = np.array(self.smpl_part_info['04_left_leg']['vertex'])\n","KeyError: '04_left_leg'\n","\t\tPre-processing: digital deformation completed...\n","the current number of sources are 2, while the pre-defined number of sources are 2. \n","\tPre-processing: successfully...\n","Step 2: running personalization on\n","#train video clips = 2\n","  0% 0/100 [00:00<?, ?it/s]Network AttLWB-SPADE was created\n","Network patch_global was created\n","Loading vgg19 from ./assets/checkpoints/losses/vgg19-dcbb9e9d.pth...\n","Loading face model from ./assets/checkpoints/losses/sphere20a_20171020.pth\n","Loading net: ./assets/checkpoints/neural_renders/AttLWB-SPADE_id_G_2020-05-18.pth\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3385: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n","100% 100/100 [02:44<00:00,  1.65s/it]\n","saving the personalized model in ./results/models/axing_1/personalized.pth\n","Step 2: personalization done, saved in ./results/models/axing_1/personalized.pth...\n","Step 3: running imitator.\n","Network AttLWB-SPADE was created\n","Loading net from ./results/models/axing_1/personalized.pth\n","Model Imitator was created\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3385: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n","  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n","pred_: 100% 1791/1791 [04:34<00:00,  6.52it/s]\n","1791it [00:24, 72.09it/s]\n","ffmpeg -y -i ./results/primitives/axing_1/synthesis/imitations/axing_1-bantangzhuyi_1.mp4.avi -i ./results/primitives/bantangzhuyi_1/processed/audio.mp3 -vcodec h264 -shortest -strict -2 ./results/primitives/axing_1/synthesis/imitations/axing_1-bantangzhuyi_1.mp4 -loglevel quiet\n","----------------------MetaOutput----------------------\n","axing_1 imitates bantangzhuyi_1 in ./results/primitives/axing_1/synthesis/imitations/axing_1-bantangzhuyi_1.mp4\n","------------------------------------------------------\n","Step 3: running imitator done.\n"]}]},{"cell_type":"code","metadata":{"id":"_t3IP0I0qtXs"},"source":["# This is a specific model name, and it will be used if you do not change it. This is the case of `trump`\n","model_id = \"orange_justice\"\n","\n","\n","\n","src_path = \"\\\"outputs\\character_1\\\"\"\n","\n","\n","\n","ref_path = \"\\\"reference\\video_1\"\"\n","\n","print(ref_path)\n","\n","!python -m iPERCore.services.run_imitator  \\\n","  --gpu_ids     $gpu_ids       \\\n","  --num_source  $num_source    \\\n","  --image_size  $image_size    \\\n","  --output_dir  $output_dir    \\\n","  --model_id    $model_id      \\\n","  --cfg_path    $cfg_path      \\\n","  --src_path    $src_path      \\\n","  --ref_path    $ref_path"],"execution_count":null,"outputs":[]}]}